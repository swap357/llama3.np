# llama3.np: NumPy Implementation of Llama3

[![Tests](https://github.com/swap357/llama3.np/actions/workflows/test_and_benchmark.yml/badge.svg)](https://github.com/swap357/llama3.np/actions/workflows/test_and_benchmark.yml)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)


A simplified NumPy implementation of the Llama3 language model with performance optimizations. This project provides both original and optimized implementations for learning, experimentation, and performance comparison.

## ğŸš€ Performance Improvements

| Component | Speedup | Notes |
|-----------|---------|-------|
| Tokenizer | **~507x** | Dictionary-based lookup instead of list.index() |
| RoPE      | **~1.07-1.5x** | Direct indexing instead of reshape/split/stack |
| Inference | **~1.01x** | End-to-end token generation speedup |

## ğŸ“‹ Requirements

- Python 3.8+
- NumPy

## ğŸ”§ Installation

```bash
git clone https://github.com/swap357/llama3.np.git
cd llama3.np
pip install -e .
```

## ğŸƒâ€â™‚ï¸ Quick Start

Run the model with default settings:

```bash
python scripts/run_llama.py --prompt "Once upon a time"
```

Use the optimized implementation:

```bash
python scripts/run_llama.py --prompt "Once upon a time" --optimized
```

Compare performance between versions:

```bash
python scripts/run_llama.py --prompt "Once upon a time" --compare
```

## ğŸ” Benchmarking

Run comprehensive benchmarks:

```bash
python scripts/run_benchmarks.py --all
```

Or test specific components:

```bash
python scripts/run_benchmarks.py --tokenization --rope
python scripts/run_benchmarks.py --inference --max-tokens 50
```

Run direct component comparisons:

```bash
python -m llama3np.benchmark.direct --prompt "Once upon a time" --iterations 50
```

Run complete model comparison:

```bash
python -m llama3np.benchmark.llama --prompt "Once upon a time" --tokens 30
```

## ğŸ”¬ Analysis Tools

Analyze model bytecode:

```bash
python analysis/bytecode/analyze_bytecode.py --funcs "apply_rotary_emb,softmax"
```

Run comprehensive profiling:

```bash
python scripts/run_analysis.py --prompt "Once upon a time" --tokens 20
```

Profile specific inference phases:

```bash
python analysis/profiling/profile_inference.py --prompt "Hello" --max-tokens 10 --phases prefill
```

## ğŸ“Š Key Findings

Our performance analysis uncovered several optimization opportunities:

1. **Tokenizer Optimization**: 
   - Original implementation used list.index() for lookups (O(n))
   - Optimized implementation uses dictionary lookup (O(1))
   - Result: Massive 507x speedup

2. **RoPE Implementation**:
   - Original implementation used complex reshape/split/stack operations
   - Optimized implementation uses direct indexing
   - Result: 7-50% speedup depending on batch size and sequence length

3. **End-to-End Performance**:
   - Tokenization is dramatically faster
   - Core inference speed shows modest 1% improvement
   - Clear separation of prefill vs. decode phases reveals different optimization needs

## ğŸ“‚ Project Structure

```
llama3.np/
â”œâ”€â”€ llama3np/                # Main package
â”‚   â”œâ”€â”€ model/               # Model implementations
â”‚   â”‚   â”œâ”€â”€ base.py          # Original implementation
â”‚   â”‚   â””â”€â”€ optimized.py     # Optimized implementation
â”‚   â”œâ”€â”€ utils/               # Utilities
â”‚   â”‚   â”œâ”€â”€ config.py        # Model configuration
â”‚   â”‚   â”œâ”€â”€ loader.py        # Weight loading utilities
â”‚   â”‚   â”œâ”€â”€ tokenizer.py     # Original tokenizer
â”‚   â”‚   â””â”€â”€ optimized_tokenizer.py # Optimized tokenizer
â”‚   â””â”€â”€ benchmark/           # Benchmarking tools
â”‚       â”œâ”€â”€ components.py    # Component-level benchmarks
â”‚       â”œâ”€â”€ end_to_end.py    # End-to-end benchmarks
â”‚       â”œâ”€â”€ direct.py        # Direct component comparison
â”‚       â””â”€â”€ llama.py         # Full model comparison
â”œâ”€â”€ scripts/                 # High-level scripts
â”‚   â”œâ”€â”€ run_llama.py         # CLI for text generation
â”‚   â”œâ”€â”€ run_benchmarks.py    # Benchmarking orchestration
â”‚   â””â”€â”€ run_analysis.py      # Analysis orchestration
â”œâ”€â”€ analysis/                # Analysis tools
â”‚   â”œâ”€â”€ bytecode/            # Bytecode analysis
â”‚   â”œâ”€â”€ profiling/           # Performance profiling
â”‚   â””â”€â”€ types/               # Type annotation analysis
â”œâ”€â”€ tests/                   # Test suite
â””â”€â”€ setup.py                 # Package installation
```

## References

Thank you to the creators of the following libraries and tools and their contributors:
- [llama2.c](https://github.com/karpathy/llama2.c) - @karpathy
- [llama.np](https://github.com/hscspring/llama.np) - @hscspring
- [modeling_llama.py](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py) - Hugging Face's Transformers

## License

MIT License